import torch
import torch.nn as nn
import torch.nn.functional as F
from ogb.nodeproppred import DglNodePropPredDataset, Evaluator
import dgl
from dgl.nn import HeteroGraphConv, GraphConv

# ===================== 0. Device =====================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# ===================== 1. Load ogbn-mag =====================
dataset = DglNodePropPredDataset(name="ogbn-mag", root="./data")
g, label_dict = dataset[0]

labels = label_dict["paper"].squeeze()

split_idx = dataset.get_idx_split()
train_idx = split_idx["train"]["paper"]
valid_idx = split_idx["valid"]["paper"]
test_idx = split_idx["test"]["paper"]

num_classes = dataset.num_classes
in_dim_paper = g.nodes["paper"].data["feat"].shape[1]

print(g)
print("paper feat shape:", g.nodes["paper"].data["feat"].shape)
print("num_classes:", num_classes)

g = g.to(device)
labels = labels.to(device)
train_idx = train_idx.to(device)
valid_idx = valid_idx.to(device)
test_idx = test_idx.to(device)

hidden_dim = 256         # hidden dimension increased
dropout_prob = 0.3       # slightly lower dropout for better convergence
max_edges_per_type = 30000  # max sampled edges per relation to control cost


# ===================== 2. Heterogeneous encoder =====================
class HeteroEncoder(nn.Module):
    def __init__(self, g, in_dim_paper, hidden_dim=256, num_layers=2, dropout=0.3):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.dropout = dropout

        # paper nodes have input features
        self.paper_in = nn.Linear(in_dim_paper, hidden_dim)

        # learnable embeddings for nodes without features
        self.embeddings = nn.ModuleDict()
        for ntype in g.ntypes:
            if "feat" not in g.nodes[ntype].data:
                num_nodes = g.num_nodes(ntype)
                self.embeddings[ntype] = nn.Embedding(num_nodes, hidden_dim)

        # multi-layer HeteroGraphConv
        self.layers = nn.ModuleList()
        for _ in range(num_layers):
            convs = {}
            for c_etype in g.canonical_etypes:
                convs[c_etype] = GraphConv(
                    in_feats=hidden_dim,
                    out_feats=hidden_dim,
                    norm="right",
                    allow_zero_in_degree=True,
                )
            self.layers.append(HeteroGraphConv(convs, aggregate="sum"))

    def forward(self, graph):
        # initial representations
        h = {}
        for ntype in graph.ntypes:
            if ntype == "paper":
                x = graph.nodes[ntype].data["feat"]
                h[ntype] = self.paper_in(x)
            elif "feat" in graph.nodes[ntype].data:
                h[ntype] = graph.nodes[ntype].data["feat"]
            else:
                h[ntype] = self.embeddings[ntype].weight

        # layer-wise propagation: update only destination types returned by layer
        for layer_id, layer in enumerate(self.layers):
            h_new = layer(graph, h)  # only destination types are returned
            for ntype in h_new:
                h[ntype] = h_new[ntype]
            if layer_id != self.num_layers - 1:
                for ntype in h:
                    h[ntype] = F.relu(h[ntype])
                    h[ntype] = F.dropout(
                        h[ntype],
                        p=self.dropout,
                        training=self.training,
                    )

        return h


# ===================== 3. Self-supervised: edge prediction loss =====================
def edge_prediction_loss(graph, h_dict, max_edges_per_type=30000):
    """
    For each canonical etype:
      - positives: existing edges
      - negatives: random node pairs of same types
      - score: dot product
      - loss: BCE with logits
    """
    total_loss = 0.0
    etype_count = 0

    for c_etype in graph.canonical_etypes:
        src_type, _, dst_type = c_etype

        # ensure both end-types have representations
        if src_type not in h_dict or dst_type not in h_dict:
            continue

        src, dst = graph.edges(etype=c_etype)
        if src.numel() == 0:
            continue

        src = src.to(device)
        dst = dst.to(device)

        num_edges = src.shape[0]
        sample_size = min(max_edges_per_type, num_edges)

        # randomly sample a subset of real edges
        perm = torch.randperm(num_edges, device=device)[:sample_size]
        pos_src = src[perm]
        pos_dst = dst[perm]

        h_src = h_dict[src_type]
        h_dst = h_dict[dst_type]

        pos_score = (h_src[pos_src] * h_dst[pos_dst]).sum(dim=-1)

        # negatives: random node pairs of same types
        neg_src = torch.randint(0, h_src.shape[0], (sample_size,), device=device)
        neg_dst = torch.randint(0, h_dst.shape[0], (sample_size,), device=device)
        neg_score = (h_src[neg_src] * h_dst[neg_dst]).sum(dim=-1)

        logits = torch.cat([pos_score, neg_score], dim=0)
        labels_ssl = torch.cat(
            [torch.ones_like(pos_score), torch.zeros_like(neg_score)],
            dim=0,
        )

        loss = F.binary_cross_entropy_with_logits(logits, labels_ssl)
        total_loss = total_loss + loss
        etype_count += 1

    if etype_count == 0:
        return torch.tensor(0.0, device=device)

    return total_loss / etype_count


# ===================== 4. Self-supervised pre-training (more epochs) =====================
encoder = HeteroEncoder(
    g=g,
    in_dim_paper=in_dim_paper,
    hidden_dim=hidden_dim,
    num_layers=2,
    dropout=dropout_prob,
).to(device)

optimizer_ssl = torch.optim.Adam(encoder.parameters(), lr=0.001, weight_decay=1e-4)

num_ssl_epochs = 80  # increased from 10 or 50

for epoch in range(1, num_ssl_epochs + 1):
    encoder.train()
    optimizer_ssl.zero_grad()

    h_dict = encoder(g)
    loss_ssl = edge_prediction_loss(g, h_dict, max_edges_per_type=max_edges_per_type)

    loss_ssl.backward()
    optimizer_ssl.step()

    if epoch % 5 == 0 or epoch == 1:
        print(f"[SSL] Epoch {epoch:03d} | Loss {loss_ssl.item():.4f}")


# ===================== 5. Down-stream model & evaluation =====================
class PaperClassifier(nn.Module):
    def __init__(self, encoder, hidden_dim, num_classes):
        super().__init__()
        self.encoder = encoder
        self.classifier = nn.Linear(hidden_dim, num_classes)

    def forward(self, graph):
        h_dict = self.encoder(graph)
        paper_h = h_dict["paper"]
        return self.classifier(paper_h)


evaluator = Evaluator(name="ogbn-mag")


def eval_split(model, idx):
    model.eval()
    with torch.no_grad():
        logits = model(g)
        y_pred = logits.argmax(dim=-1, keepdim=True)
        input_dict = {
            "y_true": labels[idx].view(-1, 1),
            "y_pred": y_pred[idx],
        }
        return evaluator.eval(input_dict)["acc"]


# ===================== 6. Linear probe (freeze encoder, more epochs) =====================
linear_model = PaperClassifier(
    encoder=encoder,  # reuse pre-trained encoder
    hidden_dim=hidden_dim,
    num_classes=num_classes,
).to(device)

# freeze encoder, train only the classification head
for p in linear_model.encoder.parameters():
    p.requires_grad = False

optimizer_lp = torch.optim.Adam(
    linear_model.classifier.parameters(),
    lr=0.01,
    weight_decay=1e-4,
)

num_lp_epochs = 40  # doubled from 20

for epoch in range(1, num_lp_epochs + 1):
    linear_model.train()
    optimizer_lp.zero_grad()

    logits = linear_model(g)
    loss = F.cross_entropy(logits[train_idx], labels[train_idx])

    loss.backward()
    optimizer_lp.step()

    if epoch % 4 == 0:
        train_acc = eval_split(linear_model, train_idx)
        valid_acc = eval_split(linear_model, valid_idx)
        print(
            f"[LP] Epoch {epoch:03d} | "
            f"Loss {loss.item():.4f} | "
            f"Train Acc {train_acc:.4f} | "
            f"Valid Acc {valid_acc:.4f}"
        )

lp_test_acc = eval_split(linear_model, test_idx)
print("Test Acc (linear probe):", lp_test_acc)


# ===================== 7. End-to-end fine-tune (from SSL weights, more epochs + lower LR) =====================
# copy encoder weights to avoid overwriting linear-probe setup
encoder_ft = HeteroEncoder(
    g=g,
    in_dim_paper=in_dim_paper,
    hidden_dim=hidden_dim,
    num_layers=2,
    dropout=dropout_prob,
).to(device)
encoder_ft.load_state_dict(encoder.state_dict())

ft_model = PaperClassifier(
    encoder=encoder_ft,
    hidden_dim=hidden_dim,
    num_classes=num_classes,
).to(device)

# end-to-end fine-tune: all parameters trainable
optimizer_ft = torch.optim.Adam(
    ft_model.parameters(),
    lr=0.0015,     # smaller LR for fine-tuning on top of SSL
    weight_decay=1e-4,
)

num_ft_epochs = 40  # doubled from 20

for epoch in range(1, num_ft_epochs + 1):
    ft_model.train()
    optimizer_ft.zero_grad()

    logits = ft_model(g)
    loss = F.cross_entropy(logits[train_idx], labels[train_idx])

    loss.backward()
    optimizer_ft.step()

    if epoch % 4 == 0:
        train_acc = eval_split(ft_model, train_idx)
        valid_acc = eval_split(ft_model, valid_idx)
        print(
            f"[FT] Epoch {epoch:03d} | "
            f"Loss {loss.item():.4f} | "
            f"Train Acc {train_acc:.4f} | "
            f"Valid Acc {valid_acc:.4f}"
        )

ft_test_acc = eval_split(ft_model, test_idx)
print("Test Acc (SSL init + end-to-end FT):", ft_test_acc)
